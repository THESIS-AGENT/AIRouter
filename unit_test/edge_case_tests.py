#!/usr/bin/env python3
"""
è¾¹ç•Œæƒ…å†µæµ‹è¯•ï¼šæµ‹è¯•ç³»ç»Ÿåœ¨å„ç§æç«¯å’Œå¼‚å¸¸æƒ…å†µä¸‹çš„è¡¨ç°
"""

import time
import json
import base64
import os
from datetime import datetime
import random
import string

from LLMwrapper import LLM_Wrapper
from LoadBalancing import LoadBalancing
from ew_config.source import (
    model_list_normal, 
    model_list_thinking, 
    model_list_mm_normal,
    model_list_function_calling
)

# æµ‹è¯•é…ç½®
TEST_IMAGE_PATH = os.path.join(os.path.dirname(__file__), "test.jpg")
RESULTS_DIR = os.path.join(os.path.dirname(__file__), "edge_case_results")
os.makedirs(RESULTS_DIR, exist_ok=True)

class EdgeCaseResults:
    """è¾¹ç•Œæµ‹è¯•ç»“æœæ”¶é›†å™¨"""
    def __init__(self):
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "tests": [],
            "summary": {
                "total": 0,
                "passed": 0,
                "failed": 0,
                "warnings": 0
            }
        }
    
    def add_test(self, test_name: str, description: str, status: str, details: dict):
        self.results["tests"].append({
            "test_name": test_name,
            "description": description,
            "status": status,
            "timestamp": datetime.now().isoformat(),
            "details": details
        })
        
        self.results["summary"]["total"] += 1
        if status == "passed":
            self.results["summary"]["passed"] += 1
        elif status == "failed":
            self.results["summary"]["failed"] += 1
        elif status == "warning":
            self.results["summary"]["warnings"] += 1
    
    def save(self, filename: str):
        filepath = os.path.join(RESULTS_DIR, filename)
        with open(filepath, "w") as f:
            json.dump(self.results, f, indent=2)
        print(f"è¾¹ç•Œæµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filepath}")

# ==================== è¾“å…¥è¾¹ç•Œæµ‹è¯• ====================

def test_extreme_inputs(results: EdgeCaseResults):
    """æµ‹è¯•æç«¯è¾“å…¥æƒ…å†µ"""
    print("\n=== æç«¯è¾“å…¥æµ‹è¯• ===")
    
    # æµ‹è¯•ç©ºè¾“å…¥
    print("\n1. ç©ºè¾“å…¥æµ‹è¯•")
    try:
        response = LLM_Wrapper.generate(
            model_name="gpt41_normal",
            prompt="",
            timeout=20
        )
        results.add_test(
            "empty_input",
            "æµ‹è¯•ç©ºå­—ç¬¦ä¸²è¾“å…¥",
            "warning" if response else "failed",
            {"response_length": len(response) if response else 0}
        )
        print(f"  ç©ºè¾“å…¥å“åº”: {response[:50] if response else 'None'}")
    except Exception as e:
        results.add_test(
            "empty_input",
            "æµ‹è¯•ç©ºå­—ç¬¦ä¸²è¾“å…¥",
            "passed",  # é¢„æœŸåº”è¯¥æŠ›å‡ºå¼‚å¸¸æˆ–å¤„ç†ç©ºè¾“å…¥
            {"error": str(e)}
        )
        print(f"  ç©ºè¾“å…¥æ­£ç¡®å¤„ç†: {str(e)[:50]}")
    
    # æµ‹è¯•è¶…é•¿è¾“å…¥
    print("\n2. è¶…é•¿è¾“å…¥æµ‹è¯•")
    long_prompt = "Please summarize this text: " + "x" * 100000  # 100kå­—ç¬¦
    try:
        start_time = time.time()
        response = LLM_Wrapper.generate(
            model_name="gpt41_normal",
            prompt=long_prompt,
            timeout=30
        )
        elapsed_time = time.time() - start_time
        results.add_test(
            "long_input",
            "æµ‹è¯•100kå­—ç¬¦çš„è¶…é•¿è¾“å…¥",
            "passed",
            {
                "input_length": len(long_prompt),
                "response_length": len(response),
                "elapsed_time": elapsed_time
            }
        )
        print(f"  è¶…é•¿è¾“å…¥å¤„ç†æˆåŠŸï¼Œè€—æ—¶: {elapsed_time:.2f}s")
    except Exception as e:
        results.add_test(
            "long_input",
            "æµ‹è¯•100kå­—ç¬¦çš„è¶…é•¿è¾“å…¥",
            "passed",  # é¢„æœŸå¯èƒ½ä¼šå¤±è´¥
            {"error": str(e)}
        )
        print(f"  è¶…é•¿è¾“å…¥å¤„ç†å¤±è´¥ï¼ˆé¢„æœŸï¼‰: {str(e)[:50]}")
    
    # æµ‹è¯•ç‰¹æ®Šå­—ç¬¦
    print("\n3. ç‰¹æ®Šå­—ç¬¦æµ‹è¯•")
    special_prompts = [
        "Test with emoji: ğŸ˜€ğŸ‰ğŸŒŸğŸ’»ğŸš€",
        "Test with unicode: Ã±Ã¡Ã©Ã­Ã³Ãº Ã‘ÃÃ‰ÃÃ“Ãš Î±Î²Î³Î´Îµ Î‘Î’Î“Î”Î•",
        "Test with control chars: \n\t\r",
        "Test with quotes: \"'`",
        "Test with HTML: <script>alert('test')</script>",
        "Test with SQL: '; DROP TABLE users; --"
    ]
    
    for i, prompt in enumerate(special_prompts):
        try:
            response = LLM_Wrapper.generate(
                model_name="gemini20_flash",
                prompt=prompt,
                timeout=20
            )
            results.add_test(
                f"special_chars_{i}",
                f"æµ‹è¯•ç‰¹æ®Šå­—ç¬¦: {prompt[:30]}",
                "passed",
                {"prompt": prompt, "response_length": len(response)}
            )
            print(f"  ç‰¹æ®Šå­—ç¬¦æµ‹è¯• {i+1}: æˆåŠŸ")
        except Exception as e:
            results.add_test(
                f"special_chars_{i}",
                f"æµ‹è¯•ç‰¹æ®Šå­—ç¬¦: {prompt[:30]}",
                "failed",
                {"prompt": prompt, "error": str(e)}
            )
            print(f"  ç‰¹æ®Šå­—ç¬¦æµ‹è¯• {i+1}: å¤±è´¥ - {str(e)[:30]}")

def test_invalid_parameters(results: EdgeCaseResults):
    """æµ‹è¯•æ— æ•ˆå‚æ•°"""
    print("\n=== æ— æ•ˆå‚æ•°æµ‹è¯• ===")
    
    # æµ‹è¯•æ— æ•ˆæ¨¡å‹å
    print("\n1. æ— æ•ˆæ¨¡å‹åæµ‹è¯•")
    invalid_models = [
        "non_existent_model",
        "gpt-5-turbo",
        "claude-100",
        "",
        None,
        123,
        ["list_model"]
    ]
    
    for model in invalid_models:
        try:
            response = LLM_Wrapper.generate(
                model_name=model,
                prompt="Test",
                timeout=20
            )
            results.add_test(
                f"invalid_model_{model}",
                f"æµ‹è¯•æ— æ•ˆæ¨¡å‹å: {model}",
                "failed",  # ä¸åº”è¯¥æˆåŠŸ
                {"model": str(model)}
            )
            print(f"  æ— æ•ˆæ¨¡å‹ {model}: æ„å¤–æˆåŠŸï¼")
        except Exception as e:
            results.add_test(
                f"invalid_model_{model}",
                f"æµ‹è¯•æ— æ•ˆæ¨¡å‹å: {model}",
                "passed",  # åº”è¯¥å¤±è´¥
                {"model": str(model), "error": str(e)}
            )
            print(f"  æ— æ•ˆæ¨¡å‹ {model}: æ­£ç¡®æ‹’ç»")
    
    # æµ‹è¯•æ— æ•ˆè¶…æ—¶å€¼
    print("\n2. æ— æ•ˆè¶…æ—¶å€¼æµ‹è¯•")
    invalid_timeouts = [-1, 0, 0.001, 10000, None, "30", [30]]
    
    for timeout in invalid_timeouts:
        try:
            # å¯¹äºæŸäº›å€¼ï¼ŒPython å¯èƒ½ä¼šè‡ªåŠ¨å¤„ç†æˆ–ä½¿ç”¨é»˜è®¤å€¼
            response = LLM_Wrapper.generate(
                model_name="gpt41_normal",
                prompt="Quick test",
                timeout=timeout
            )
            # å¦‚æœæˆåŠŸäº†ï¼Œæ£€æŸ¥æ˜¯å“ªç§æƒ…å†µ
            if timeout in [0.001]:  # å¤ªå°çš„è¶…æ—¶å€¼å¯èƒ½å¯¼è‡´è¶…æ—¶
                status = "warning"
            elif timeout in [10000]:  # å¤ªå¤§çš„è¶…æ—¶å€¼å¯èƒ½è¢«æ¥å—
                status = "warning"
            elif isinstance(timeout, (int, float)) and timeout > 0:  # æœ‰æ•ˆçš„æ•°å€¼
                status = "warning"
            else:
                status = "failed"  # ä¸åº”è¯¥æˆåŠŸ
            
            results.add_test(
                f"invalid_timeout_{timeout}",
                f"æµ‹è¯•æ— æ•ˆè¶…æ—¶å€¼: {timeout}",
                status,
                {"timeout": str(timeout), "note": "Request succeeded with this timeout"}
            )
            print(f"  è¶…æ—¶å€¼ {timeout}: å¤„ç†æˆåŠŸ")
        except Exception as e:
            # å¯¹äºçœŸæ­£æ— æ•ˆçš„è¶…æ—¶å€¼ï¼Œåº”è¯¥æŠ›å‡ºå¼‚å¸¸
            expected_errors = [None, "30", [30], -1, 0]  # è¿™äº›åº”è¯¥å¤±è´¥
            if timeout in expected_errors:
                status = "passed"
            else:
                status = "warning"
                
            results.add_test(
                f"invalid_timeout_{timeout}",
                f"æµ‹è¯•æ— æ•ˆè¶…æ—¶å€¼: {timeout}",
                status,
                {"timeout": str(timeout), "error": str(e)[:100]}
            )
            print(f"  è¶…æ—¶å€¼ {timeout}: æ­£ç¡®æ‹’ç»")
    
    # æµ‹è¯•æ— æ•ˆæ¯”ä¾‹å‚æ•°
    print("\n3. æ— æ•ˆæ¯”ä¾‹å‚æ•°æµ‹è¯•")
    invalid_proportions = [
        (-1, 50),
        (50, -1),
        (0, 0),
        (1000, 1000),
        (None, 50),
        ("50", "50")
    ]
    
    for input_prop, output_prop in invalid_proportions:
        try:
            response = LLM_Wrapper.generate(
                model_name="gpt41_normal",
                prompt="Test proportions",
                input_proportion=input_prop,
                output_proportion=output_prop,
                timeout=20
            )
            results.add_test(
                f"invalid_proportions_{input_prop}_{output_prop}",
                f"æµ‹è¯•æ— æ•ˆæ¯”ä¾‹: {input_prop}/{output_prop}",
                "warning",  # å¯èƒ½æœ‰é»˜è®¤å¤„ç†
                {"input": str(input_prop), "output": str(output_prop)}
            )
            print(f"  æ¯”ä¾‹ {input_prop}/{output_prop}: æœ‰é»˜è®¤å¤„ç†")
        except Exception as e:
            results.add_test(
                f"invalid_proportions_{input_prop}_{output_prop}",
                f"æµ‹è¯•æ— æ•ˆæ¯”ä¾‹: {input_prop}/{output_prop}",
                "passed",
                {"error": str(e)}
            )
            print(f"  æ¯”ä¾‹ {input_prop}/{output_prop}: é”™è¯¯å¤„ç†")

def test_multimodal_edge_cases(results: EdgeCaseResults):
    """æµ‹è¯•å¤šæ¨¡æ€è¾¹ç•Œæƒ…å†µ"""
    print("\n=== å¤šæ¨¡æ€è¾¹ç•Œæµ‹è¯• ===")
    
    # æµ‹è¯•æ— æ•ˆå›¾åƒæ•°æ®
    print("\n1. æ— æ•ˆå›¾åƒæ•°æ®æµ‹è¯•")
    invalid_images = [
        "",  # ç©ºå­—ç¬¦ä¸²
        "not_base64_encoded",  # ébase64
        "aGVsbG8=",  # æœ‰æ•ˆbase64ä½†ä¸æ˜¯å›¾åƒ
        None,  # Noneå€¼
    ]
    
    for i, img_data in enumerate(invalid_images):
        try:
            response = LLM_Wrapper.generate_mm(
                model_name="gpt41_normal_mm",
                prompt="Describe this image",
                img_base64=img_data,
                timeout=20
            )
            results.add_test(
                f"invalid_image_{i}",
                f"æµ‹è¯•æ— æ•ˆå›¾åƒæ•°æ®ç±»å‹ {i}",
                "failed",  # ä¸åº”è¯¥æˆåŠŸ
                {"image_type": type(img_data).__name__}
            )
            print(f"  æ— æ•ˆå›¾åƒ {i}: æ„å¤–æˆåŠŸï¼")
        except Exception as e:
            results.add_test(
                f"invalid_image_{i}",
                f"æµ‹è¯•æ— æ•ˆå›¾åƒæ•°æ®ç±»å‹ {i}",
                "passed",
                {"error": str(e)[:100]}
            )
            print(f"  æ— æ•ˆå›¾åƒ {i}: æ­£ç¡®æ‹’ç»")
    
    # æµ‹è¯•è¶…å¤§å›¾åƒ
    print("\n2. è¶…å¤§å›¾åƒæµ‹è¯•")
    # åˆ›å»ºä¸€ä¸ªè¶…å¤§çš„å‡å›¾åƒæ•°æ®
    large_image = base64.b64encode(b"x" * 10000000).decode("utf-8")  # 10MB
    try:
        response = LLM_Wrapper.generate_mm(
            model_name="gpt41_normal_mm",
            prompt="Describe this large image",
            img_base64=large_image,
            timeout=30
        )
        results.add_test(
            "large_image",
            "æµ‹è¯•10MBå¤§å›¾åƒ",
            "warning",
            {"image_size": len(large_image)}
        )
        print("  è¶…å¤§å›¾åƒ: å¤„ç†æˆåŠŸï¼ˆå¯èƒ½æœ‰æ€§èƒ½å½±å“ï¼‰")
    except Exception as e:
        results.add_test(
            "large_image",
            "æµ‹è¯•10MBå¤§å›¾åƒ",
            "passed",
            {"error": str(e)[:100]}
        )
        print("  è¶…å¤§å›¾åƒ: æ­£ç¡®é™åˆ¶")
    
    # æµ‹è¯•é”™è¯¯çš„æ¨¡å‹ç”¨äºå¤šæ¨¡æ€
    print("\n3. é”™è¯¯æ¨¡å‹ç±»å‹æµ‹è¯•")
    try:
        response = LLM_Wrapper.generate_mm(
            model_name="gpt41_normal",  # éå¤šæ¨¡æ€æ¨¡å‹
            prompt="Describe image",
            img_base64="valid_base64_here",
            timeout=20
        )
        results.add_test(
            "wrong_model_type",
            "ä½¿ç”¨éå¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€è°ƒç”¨",
            "failed",
            {}
        )
        print("  é”™è¯¯æ¨¡å‹ç±»å‹: æ„å¤–æˆåŠŸï¼")
    except ValueError as e:
        results.add_test(
            "wrong_model_type",
            "ä½¿ç”¨éå¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€è°ƒç”¨",
            "passed",
            {"error": str(e)}
        )
        print("  é”™è¯¯æ¨¡å‹ç±»å‹: æ­£ç¡®æ‹’ç»")

def test_function_calling_edge_cases(results: EdgeCaseResults):
    """æµ‹è¯•å‡½æ•°è°ƒç”¨è¾¹ç•Œæƒ…å†µ"""
    print("\n=== å‡½æ•°è°ƒç”¨è¾¹ç•Œæµ‹è¯• ===")
    
    # æµ‹è¯•æ— æ•ˆå·¥å…·å®šä¹‰
    print("\n1. æ— æ•ˆå·¥å…·å®šä¹‰æµ‹è¯•")
    invalid_tools = [
        [],  # ç©ºåˆ—è¡¨
        None,  # None
        "not a list",  # å­—ç¬¦ä¸²
        [{"invalid": "structure"}],  # é”™è¯¯ç»“æ„
        [{"type": "function"}],  # ç¼ºå°‘functionå­—æ®µ
    ]
    
    for i, tools in enumerate(invalid_tools):
        try:
            response = LLM_Wrapper.function_calling(
                model_name="claude37_normal",
                prompt="Use a tool",
                tools=tools,
                timeout=20
            )
            results.add_test(
                f"invalid_tools_{i}",
                f"æµ‹è¯•æ— æ•ˆå·¥å…·å®šä¹‰ {i}",
                "warning",
                {"tools_type": type(tools).__name__}
            )
            print(f"  æ— æ•ˆå·¥å…· {i}: å¤„ç†æˆåŠŸ")
        except Exception as e:
            results.add_test(
                f"invalid_tools_{i}",
                f"æµ‹è¯•æ— æ•ˆå·¥å…·å®šä¹‰ {i}",
                "passed",
                {"error": str(e)[:100]}
            )
            print(f"  æ— æ•ˆå·¥å…· {i}: æ­£ç¡®å¤„ç†")
    
    # æµ‹è¯•è¶…å¤šå·¥å…·
    print("\n2. å¤§é‡å·¥å…·æµ‹è¯•")
    many_tools = []
    for i in range(100):  # 100ä¸ªå·¥å…·
        many_tools.append({
            "type": "function",
            "function": {
                "name": f"tool_{i}",
                "description": f"Tool number {i}",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "param": {"type": "string"}
                    }
                }
            }
        })
    
    try:
        start_time = time.time()
        response = LLM_Wrapper.function_calling(
            model_name="claude37_normal",
            prompt="Use tool_50",
            tools=many_tools,
            timeout=30
        )
        elapsed_time = time.time() - start_time
        results.add_test(
            "many_tools",
            "æµ‹è¯•100ä¸ªå·¥å…·å®šä¹‰",
            "passed",
            {
                "num_tools": len(many_tools),
                "elapsed_time": elapsed_time,
                "has_tool_calls": "tool_calls" in response
            }
        )
        print(f"  å¤§é‡å·¥å…·æµ‹è¯•: æˆåŠŸï¼Œè€—æ—¶ {elapsed_time:.2f}s")
    except Exception as e:
        results.add_test(
            "many_tools",
            "æµ‹è¯•100ä¸ªå·¥å…·å®šä¹‰",
            "warning",
            {"error": str(e)}
        )
        print(f"  å¤§é‡å·¥å…·æµ‹è¯•: å¤±è´¥ - {str(e)[:50]}")

def test_concurrent_edge_cases(results: EdgeCaseResults):
    """æµ‹è¯•å¹¶å‘è¾¹ç•Œæƒ…å†µ"""
    print("\n=== å¹¶å‘è¾¹ç•Œæµ‹è¯• ===")
    
    # æµ‹è¯•åŒä¸€æ¨¡å‹çš„å¿«é€Ÿè¿ç»­è°ƒç”¨
    print("\n1. å¿«é€Ÿè¿ç»­è°ƒç”¨æµ‹è¯•")
    model = "gpt41_normal"
    rapid_results = []
    
    for i in range(10):
        try:
            # ä¸ç­‰å¾…ï¼Œç«‹å³å‘é€ä¸‹ä¸€ä¸ªè¯·æ±‚
            start_time = time.time()
            response = LLM_Wrapper.generate(
                model_name=model,
                prompt=f"Rapid test {i}",
                timeout=20
            )
            elapsed_time = time.time() - start_time
            rapid_results.append({
                "index": i,
                "success": True,
                "time": elapsed_time
            })
        except Exception as e:
            rapid_results.append({
                "index": i,
                "success": False,
                "error": str(e)[:50]
            })
    
    success_count = sum(1 for r in rapid_results if r["success"])
    results.add_test(
        "rapid_sequential",
        "å¿«é€Ÿè¿ç»­è°ƒç”¨åŒä¸€æ¨¡å‹10æ¬¡",
        "passed" if success_count >= 8 else "warning",
        {
            "total": len(rapid_results),
            "successful": success_count,
            "failed": len(rapid_results) - success_count
        }
    )
    print(f"  å¿«é€Ÿè¿ç»­è°ƒç”¨: {success_count}/10 æˆåŠŸ")
    
    # æµ‹è¯•æ¨¡å‹åˆ—è¡¨è¾¹ç•Œ
    print("\n2. æ¨¡å‹åˆ—è¡¨è¾¹ç•Œæµ‹è¯•")
    
    # ç©ºåˆ—è¡¨
    try:
        response = LLM_Wrapper.generate_fromTHEbest(
            model_list=[],
            prompt="Test empty list",
            timeout=20
        )
        results.add_test(
            "empty_model_list",
            "æµ‹è¯•ç©ºæ¨¡å‹åˆ—è¡¨",
            "failed",
            {}
        )
        print("  ç©ºæ¨¡å‹åˆ—è¡¨: æ„å¤–æˆåŠŸï¼")
    except Exception as e:
        results.add_test(
            "empty_model_list",
            "æµ‹è¯•ç©ºæ¨¡å‹åˆ—è¡¨",
            "passed",
            {"error": str(e)}
        )
        print("  ç©ºæ¨¡å‹åˆ—è¡¨: æ­£ç¡®æ‹’ç»")
    
    # åŒ…å«æ— æ•ˆæ¨¡å‹çš„åˆ—è¡¨
    mixed_models = ["gpt41_normal", "invalid_model", "gemini20_flash"]
    try:
        response = LLM_Wrapper.generate_fromTHEbest(
            model_list=mixed_models,
            prompt="Test mixed models",
            timeout=20
        )
        results.add_test(
            "mixed_valid_invalid_models",
            "æµ‹è¯•åŒ…å«æ— æ•ˆæ¨¡å‹çš„åˆ—è¡¨",
            "failed",
            {"models": mixed_models}
        )
        print("  æ··åˆæ¨¡å‹åˆ—è¡¨: æ„å¤–æˆåŠŸï¼")
    except Exception as e:
        results.add_test(
            "mixed_valid_invalid_models",
            "æµ‹è¯•åŒ…å«æ— æ•ˆæ¨¡å‹çš„åˆ—è¡¨",
            "passed",
            {"error": str(e)[:100]}
        )
        print("  æ··åˆæ¨¡å‹åˆ—è¡¨: æ­£ç¡®å¤„ç†")

def test_load_balancing_edge_cases(results: EdgeCaseResults):
    """æµ‹è¯•è´Ÿè½½å‡è¡¡è¾¹ç•Œæƒ…å†µ"""
    print("\n=== è´Ÿè½½å‡è¡¡è¾¹ç•Œæµ‹è¯• ===")
    
    lb = LoadBalancing()
    
    # æµ‹è¯•æ— å¥åº·æ•°æ®çš„æƒ…å†µ
    print("\n1. æ— å¥åº·æ•°æ®æµ‹è¯•")
    # è¿™ä¸ªæµ‹è¯•ä¾èµ–äºç³»ç»Ÿåˆšå¯åŠ¨æ—¶çš„çŠ¶æ€
    
    # æµ‹è¯•æ‰€æœ‰æºéƒ½å¤±è´¥çš„æƒ…å†µ
    print("\n2. é™çº§å¤„ç†æµ‹è¯•")
    # ä½¿ç”¨ä¸€ä¸ªå¯èƒ½åœ¨å¤šä¸ªæºä¸Šéƒ½æœ‰é—®é¢˜çš„æ¨¡å‹
    problematic_models = ["claude4_opus", "claude4_sonnet"]
    
    for model in problematic_models:
        try:
            config = lb.get_config(model, "fast_first", 60, 40)
            results.add_test(
                f"degraded_model_{model}",
                f"æµ‹è¯•å¯èƒ½é™çº§çš„æ¨¡å‹: {model}",
                "passed",
                {
                    "model": model,
                    "main_source": config[0],
                    "backup_source": config[3]
                }
            )
            print(f"  é™çº§æ¨¡å‹ {model}: æ‰¾åˆ°é…ç½®")
        except Exception as e:
            results.add_test(
                f"degraded_model_{model}",
                f"æµ‹è¯•å¯èƒ½é™çº§çš„æ¨¡å‹: {model}",
                "warning",
                {"error": str(e)}
            )
            print(f"  é™çº§æ¨¡å‹ {model}: é…ç½®å¤±è´¥")
    
    # æµ‹è¯•æç«¯æ¯”ä¾‹
    print("\n3. æç«¯æ¯”ä¾‹æµ‹è¯•")
    extreme_proportions = [
        (1, 99),
        (99, 1),
        (0, 100),
        (100, 0)
    ]
    
    for input_prop, output_prop in extreme_proportions:
        try:
            config = lb.get_config("gpt41_normal", "cheap_first", input_prop, output_prop)
            results.add_test(
                f"extreme_proportions_{input_prop}_{output_prop}",
                f"æµ‹è¯•æç«¯æ¯”ä¾‹: {input_prop}/{output_prop}",
                "passed",
                {
                    "proportions": f"{input_prop}/{output_prop}",
                    "main_source": config[0]
                }
            )
            print(f"  æç«¯æ¯”ä¾‹ {input_prop}/{output_prop}: æˆåŠŸ")
        except Exception as e:
            results.add_test(
                f"extreme_proportions_{input_prop}_{output_prop}",
                f"æµ‹è¯•æç«¯æ¯”ä¾‹: {input_prop}/{output_prop}",
                "failed",
                {"error": str(e)}
            )
            print(f"  æç«¯æ¯”ä¾‹ {input_prop}/{output_prop}: å¤±è´¥")

# ==================== ä¸»æµ‹è¯•å‡½æ•° ====================

def run_edge_case_tests():
    """è¿è¡Œæ‰€æœ‰è¾¹ç•Œæµ‹è¯•"""
    results = EdgeCaseResults()
    
    print("="*60)
    print("å¼€å§‹è¾¹ç•Œæƒ…å†µæµ‹è¯•")
    print("="*60)
    
    # è¾“å…¥è¾¹ç•Œæµ‹è¯•
    test_extreme_inputs(results)
    
    # å‚æ•°éªŒè¯æµ‹è¯•
    test_invalid_parameters(results)
    
    # å¤šæ¨¡æ€è¾¹ç•Œæµ‹è¯•
    test_multimodal_edge_cases(results)
    
    # å‡½æ•°è°ƒç”¨è¾¹ç•Œæµ‹è¯•
    test_function_calling_edge_cases(results)
    
    # å¹¶å‘è¾¹ç•Œæµ‹è¯•
    test_concurrent_edge_cases(results)
    
    # è´Ÿè½½å‡è¡¡è¾¹ç•Œæµ‹è¯•
    test_load_balancing_edge_cases(results)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results.save(f"edge_case_test_results_{timestamp}.json")
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*60)
    print("è¾¹ç•Œæµ‹è¯•æ€»ç»“")
    print("="*60)
    
    summary = results.results["summary"]
    print(f"æ€»æµ‹è¯•æ•°: {summary['total']}")
    print(f"é€šè¿‡: {summary['passed']}")
    print(f"å¤±è´¥: {summary['failed']}")
    print(f"è­¦å‘Š: {summary['warnings']}")
    print(f"é€šè¿‡ç‡: {summary['passed']/summary['total']*100:.1f}%")
    
    # æ‰“å°å¤±è´¥çš„æµ‹è¯•
    if summary['failed'] > 0:
        print("\nå¤±è´¥çš„æµ‹è¯•:")
        for test in results.results["tests"]:
            if test["status"] == "failed":
                print(f"  - {test['test_name']}: {test['description']}")

if __name__ == "__main__":
    run_edge_case_tests() 