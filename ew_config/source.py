# éæ¨ç†ç±»å¤§è¯­è¨€æ¨¡å‹.
# æŒ‰ç…§å“åº”æ—¶é—´ä»å¿«åˆ°æ…¢æ’åº
model_list_normal = [
    "gpt41_normal",      # openai|gpt41_normal: 1.87, openrouter|gpt41_normal: 1.65
    "gpt41_mini",        # openai|gpt41_mini: 2.08, openrouter|gpt41_mini: 2.01
    "gemini20_flash",    # google|gemini20_flash: 1.78, deepinfra|gemini20_flash: 2.17, openrouter|gemini20_flash: 2.05
    "llama4_maverick",   # deepinfra|llama4_maverick: 1.65, togetherai|llama4_maverick: 1.89, openrouter|llama4_maverick: 1.97
    "llama4_scout",      # togetherai|llama4_scout: 1.39, openrouter|llama4_scout: 2.28, deepinfra|llama4_scout: 3.01
    "gemini25_flash",    # google|gemini25_flash: 2.17, openrouter|gemini25_flash: 2.17, deepinfra|gemini25_flash: 3.14
    "claude37_normal",   # anthropic|claude37_normal: 2.37, deepinfra|claude37_normal: 2.13, openrouter|claude37_normal: 2.47
    "gemini25_pro",      # google|gemini25_pro: 6.47, deepinfra|gemini25_pro: 3.69, openrouter|gemini25_pro: 7.49
    "gpto4_mini_high",   # openai|gpto4_mini_high: 6.83, openrouter|gpto4_mini_high: 15.22
    "qwen25_72b_instruct", # togetherai|qwen25_72b_instruct: 2.24, deepinfra|qwen25_72b_instruct: 6.29, openrouter|qwen25_72b_instruct: 7.59
    "claude4_opus",      # anthropic|claude4_opus: 3.51, openrouter|claude4_opus: 3.81
    "claude4_sonnet",     # anthropic|claude4_sonnet: 2.84, openrouter|claude4_sonnet: 2.62
    "grok3",
    "grok3_mini",
    "grok4"
]

# æ¨ç†ç±»å¤§è¯­è¨€æ¨¡å‹.
# æŒ‰ç…§å“åº”æ—¶é—´ä»å¿«åˆ°æ…¢æ’åº
model_list_thinking = [
    "gemini25_flash_thinking",  # openrouter|gemini25_flash_thinking: 3.45
    "claude37_thinking",        # openrouter|claude37_thinking: 4.38
    "qwen3_30b_moe",            # æš‚æ— å»¶æ—¶æ•°æ®
    "qwen3_235b_moe",           # togetherai|qwen3_235b_moe: 27.31
    "claude4_opus_thinking",    # æš‚æ— å»¶æ—¶æ•°æ®
    "claude4_sonnet_thinking"   # æš‚æ— å»¶æ—¶æ•°æ®
]

# éæ¨ç†ç±»å¤šæ¨¡æ€å¤§æ¨¡å‹.
# æŒ‰ç…§å“åº”æ—¶é—´ä»å¿«åˆ°æ…¢æ’åº
model_list_mm_normal = [
    "gpt41_normal_mm",      # openai|gpt41_normal_mm: 1.36, openrouter|gpt41_normal_mm: 1.35
    "llama4_maverick_mm",   # togetherai|llama4_maverick_mm: 1.05, openrouter|llama4_maverick_mm: 1.40
    "llama4_scout_mm",      # togetherai|llama4_scout_mm: 1.04, openrouter|llama4_scout_mm: 1.20
    "gemini25_flash_mm",    # openrouter|gemini25_flash_mm: 1.69, google|gemini25_flash_mm: 2.96
    "llama32_vl_90b_instruct",  # deepinfra|llama32_vl_90b_instruct: 2.31, togetherai|llama32_vl_90b_instruct: 2.70, openrouter|llama32_vl_90b_instruct: 3.22
    "gemini25_pro_mm",      # google|gemini25_pro_mm: 20.23, openrouter|gemini25_pro_mm: 20.52
    "gpto4_mini_high_mm",   # openai|gpto4_mini_high_mm: 3.23, openrouter|gpto4_mini_high_mm: 7.77
    "qwen25_vl_72b_instruct",  # togetherai|qwen25_vl_72b_instruct: 5.94, openrouter|qwen25_vl_72b_instruct: 11.14
    "claude4_sonnet_mm",     # anthropic|claude4_sonnet_mm: 2.34, openrouter|claude4_sonnet_mm: 2.57
    "grok3_mm",
    "grok4_mm"
]

# æ¨ç†ç±»å¤šæ¨¡æ€å¤§æ¨¡å‹, ä¾‹å¦‚QvQ-72B, chatGPT o3ä»¥åŠo4-mini
model_list_mm_thinking = []

# PDFæ–‡æ¡£å¤„ç†æ¨¡å‹åˆ—è¡¨ï¼Œåªæ”¯æŒGoogleæºçš„æ¨¡å‹
model_list_doc_normal = [
    "gemini20_flash",      # google|gemini20_flash: æ”¯æŒPDFå¤„ç†
    "gemini25_flash",      # google|gemini25_flash: æ”¯æŒPDFå¤„ç†
    "gemini25_pro",        # google|gemini25_pro: æ”¯æŒPDFå¤„ç†
]

model_list_function_calling = ["claude37_normal", "gemini25_pro", "claude4_opus", "claude4_sonnet", "gemini25_flash", "grok4"]

model_list_embedding = ["BAAI/bge-en-icl", "BAAI/bge-large-en-v1.5", "BAAI/bge-m3", "BAAI/bge-m3-multi", "Qwen/Qwen3-Embedding-0.6B", "Qwen/Qwen3-Embedding-4B", "Qwen/Qwen3-Embedding-8B"]
model_list_reranker = ["Qwen/Qwen3-Reranker-0.6B", "Qwen/Qwen3-Reranker-4B", "Qwen/Qwen3-Reranker-8B"]

# å¥åº·æ£€æµ‹å±è”½æ¨¡å‹æ¸…å• - è¿™äº›æ¨¡å‹ä»ç„¶å¯ä»¥è¢«LLM_Wrapperä½¿ç”¨ï¼Œä½†ä¸è¿›è¡Œå¥åº·æ£€æµ‹ä»¥é¿å…è´¹ç”¨æ¶ˆè€—
# ä¸»è¦ç”¨äºå±è”½é«˜ä»·æ¨¡å‹å¦‚claude4_opusç­‰ï¼Œé¿å…å¥åº·æ£€æµ‹äº§ç”Ÿä¸å¿…è¦çš„è´¹ç”¨
# 
# å±è”½é€»è¾‘è¯´æ˜ï¼š
# 1. ç²¾ç¡®å±è”½ï¼šå¦‚æœæ·»åŠ  "model_mm"ï¼Œåªå±è”½å¤šæ¨¡æ€ç‰ˆæœ¬
# 2. å±‚çº§å±è”½ï¼šå¦‚æœæ·»åŠ  "model"ï¼Œå±è”½è¯¥æ¨¡å‹çš„æ‰€æœ‰ç‰ˆæœ¬ï¼ˆåŒ…æ‹¬å¤šæ¨¡æ€å’Œéå¤šæ¨¡æ€ï¼‰
health_check_blacklist = [
    # é«˜ä»·æ¨ç†æ¨¡å‹
    "claude4_opus",           # éå¸¸æ˜‚è´µçš„æ¨¡å‹ï¼ˆå±‚çº§å±è”½ï¼šåŒæ—¶å±è”½claude4_opuså’Œclaude4_opus_mmï¼‰
    "claude4_opus_thinking",  # æ¨ç†ç‰ˆæœ¬æ›´æ˜‚è´µ
    "claude4_sonnet_thinking", # æ¨ç†ç‰ˆæœ¬ç›¸å¯¹æ˜‚è´µ
]


def is_model_health_check_blacklisted(model_name):
    """
    æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨å¥åº·æ£€æµ‹å±è”½æ¸…å•ä¸­
    
    æ”¯æŒå±‚çº§å±è”½é€»è¾‘ï¼š
    1. ç²¾ç¡®åŒ¹é…ï¼šå¦‚æœæ¨¡å‹åç§°ç›´æ¥åœ¨å±è”½æ¸…å•ä¸­ï¼Œåˆ™è¢«å±è”½
    2. å±‚çº§å±è”½ï¼šå¦‚æœæ˜¯å¤šæ¨¡æ€æ¨¡å‹(_mmåç¼€)ï¼Œè¿˜éœ€æ£€æŸ¥å…¶åŸºç¡€æ¨¡å‹æ˜¯å¦è¢«å±è”½
    
    Args:
        model_name (str): è¦æ£€æŸ¥çš„æ¨¡å‹åç§°
        
    Returns:
        bool: å¦‚æœæ¨¡å‹åº”è¯¥è¢«å±è”½è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        
    Examples:
        # å‡è®¾å±è”½æ¸…å•åŒ…å« ["claude4_opus", "gemini25_pro_mm"]
        is_model_health_check_blacklisted("claude4_opus")      # True (ç²¾ç¡®åŒ¹é…)
        is_model_health_check_blacklisted("claude4_opus_mm")   # True (å±‚çº§å±è”½ï¼Œå› ä¸ºclaude4_opusè¢«å±è”½)
        is_model_health_check_blacklisted("gemini25_pro")      # False (åŸºç¡€æ¨¡å‹æœªå±è”½)
        is_model_health_check_blacklisted("gemini25_pro_mm")   # True (ç²¾ç¡®åŒ¹é…)
    """
    # 1. ç²¾ç¡®åŒ¹é…æ£€æŸ¥
    if model_name in health_check_blacklist:
        return True
    
    # 2. å±‚çº§å±è”½æ£€æŸ¥ï¼ˆä»…å¯¹å¤šæ¨¡æ€æ¨¡å‹ï¼‰
    if model_name.endswith("_mm"):
        base_model_name = model_name[:-3]  # å»æ‰"_mm"åç¼€
        if base_model_name in health_check_blacklist:
            return True
    
    return False


# é€šè¿‡source_configå¯ä»¥åˆ¤æ–­è¿™ä¸ªæºæ˜¯å¦æ”¯æŒopenai API-SDKè°ƒç”¨, ä»¥åŠCURLè°ƒç”¨, å¦‚æœä¸å­˜åœ¨ç›¸å…³å­—æ®µ, åˆ™æ˜¯æš‚ä¸æ”¯æŒ.

source_config = {"openrouter": {"openai": "https://openrouter.ai/api/v1", "curl": "https://openrouter.ai/api/v1/chat/completions"},
                "deepinfra": {"openai": "https://api.deepinfra.com/v1/openai", "curl": "https://api.deepinfra.com/v1/openai/chat/completions"},
                "deerapi": {"openai": "https://api.deerapi.com/v1", "curl": "https://api.deerapi.com/v1/chat/completions"},
                "togetherai": {"openai": "https://api.together.xyz/v1", "curl": "https://api.together.xyz/v1/chat/completions"},
                # å¢åŠ äº†å››ä¸ªå®˜æ–¹æº.
                "google": {"openai": "https://generativelanguage.googleapis.com/v1beta/openai/", "curl": "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions"},
                "anthropic": {"openai": "https://api.anthropic.com/v1/", "curl": "https://api.anthropic.com/v1/complete"},
                "openai": {"openai": "https://api.openai.com/v1", "curl": "https://api.openai.com/v1/responses"}
                }

# è¯¥æ’åç”¨äºå†·å¯åŠ¨æ—¶çš„å‚è€ƒä¼˜å…ˆçº§.
source_ranking = {"google": 1, "anthropic": 2, "openai": 3, "openrouter": 4, "deepinfra": 5, "deerapi": 6, "togetherai": 7}

# é€»è¾‘æ˜¯, å¦‚æœä¸€ä¸ªæ¨¡å‹çš„è‡ªå®šä¹‰å‘½å, ä»¥"_mm"ç»“å°¾, ä¸”valueä¸ºNone, åˆ™æŸ¥æ‰¾å…¶åŒåçš„éå¤šæ¨¡æ€æ¥å£.
# å³, äºŒè€…ä¸ºåŒä¸€ä¸ªæ¨¡å‹æ¥å£.
# å¦‚æœå•çº¯valueä¸ºNone, åˆ™ä»£è¡¨å½“å‰æºå­˜åœ¨è¯¥æ¨¡å‹.

source_mapping = {
    # https://openrouter.ai/models

    "openrouter": {
        "qwen25_72b_instruct": "qwen/qwen-2.5-72b-instruct",
        "qwen25_vl_72b_instruct": None,
        "llama4_maverick": "meta-llama/llama-4-maverick",
        "llama4_scout": "meta-llama/llama-4-scout",
        "llama32_vl_90b_instruct": None,
        "claude37_normal": "anthropic/claude-3.7-sonnet",
        "claude37_thinking": "anthropic/claude-3.7-sonnet:thinking",
        "gemini20_flash": None,
        "gemini25_flash": "google/gemini-2.5-flash",
        "gemini25_pro": "google/gemini-2.5-pro",
        "gemini25_flash_thinking": "google/gemini-2.5-flash-preview:thinking",
        "gpt41_normal": "openai/gpt-4.1",
        "gpt41_mini": "openai/gpt-4.1-mini",
        "gpto4_mini_high": "openai/o4-mini-high",
        "gemini25_pro_mm": None,
        "gemini25_flash_mm": None,
        "gpt41_normal_mm": None,
        "gpto4_mini_high_mm": None,
        "llama4_maverick_mm": None,
        "llama4_scout_mm": None,
        "qwen3_235b_moe": None, 
        "qwen3_30b_moe": None,
        "claude4_opus": "anthropic/claude-opus-4",
        "claude4_sonnet": "anthropic/claude-sonnet-4",
        "claude4_sonnet_thinking": None,
        "claude4_opus_thinking": None,
        "claude4_sonnet_mm": None,
        "grok3": "x-ai/grok-3",
        "grok3_mini": "x-ai/grok-3-mini",
        "grok4": "x-ai/grok-4"
    },
    # https://deepinfra.com/models

    "deepinfra": {
        "qwen25_72b_instruct": "Qwen/Qwen2.5-72B-Instruct",
        "qwen25_vl_72b_instruct": None,
        "llama4_maverick": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
        "llama4_scout": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "llama32_vl_90b_instruct": "meta-llama/Llama-3.2-90B-Vision-Instruct",
        "claude37_normal": "anthropic/claude-3-7-sonnet-latest",
        "claude37_thinking": None,
        "gemini20_flash": "google/gemini-2.0-flash-001",
        "gemini25_flash": "google/gemini-2.5-flash",
        "gemini25_pro": "google/gemini-2.5-pro",
        "gemini25_flash_thinking": None,
        "gpt41_normal": None,
        "gpt41_mini": None,
        "gpto4_mini_high": None,
        "gemini25_pro_mm": None,
        "gemini25_flash_mm": None,
        "gpt41_normal_mm": None,
        "gpto4_mini_high_mm": None,
        "llama4_maverick_mm": None,
        "llama4_scout_mm": None,
        "qwen3_235b_moe": None, 
        "qwen3_30b_moe": None,
        "claude4_opus": None,
        "claude4_sonnet": None,
        "claude4_sonnet_thinking": None,
        "claude4_opus_thinking": None,
        "claude4_sonnet_mm": None,
        "grok3": None,
        "grok3_mini": None,
        "grok4": None
    },
    # https://api.deerapi.com/pricing

    "deerapi": {
        "qwen25_72b_instruct": "Qwen2.5-72B-Instruct-128K",
        "qwen25_vl_72b_instruct": None,
        "llama4_maverick": "llama-4-maverick",
        "llama4_scout": None,  # ç§»é™¤ï¼šæµ‹è¯•å¤±è´¥ï¼Œä¸å¯ç”¨
        "llama32_vl_90b_instruct": None,
        "claude37_normal": "claude-3-7-sonnet-latest",
        "claude37_thinking": "claude-3-7-sonnet-thinking",
        "gemini20_flash": "gemini-2.0-flash",
        "gemini25_flash": "gemini-2.5-flash",
        "gemini25_pro": "gemini-2.5-pro-preview-06-05",
        "gemini25_flash_thinking": None,
        "gpt41_normal": "gpt-4.1",
        "gpt41_mini": "gpt-4.1-mini",
        "gpto4_mini_high": None,
        "gemini25_pro_mm": None,
        "gemini25_flash_mm": None,
        "gpt41_normal_mm": None,
        "gpto4_mini_high_mm": None,
        "llama4_maverick_mm": None,
        "llama4_scout_mm": None,  # ç§»é™¤ï¼šåŸºäºllama4_scoutï¼ŒåŒæ ·ä¸å¯ç”¨
        "qwen3_235b_moe": "qwen3-235b-a22b", 
        "qwen3_30b_moe": "qwen3-30b-a3b",
        "claude4_opus": "claude-opus-4-20250514",
        "claude4_sonnet": "claude-sonnet-4-20250514",
        "claude4_sonnet_thinking": "claude-sonnet-4-20250514-thinking",
        "claude4_opus_thinking": "claude-opus-4-20250514-thinking",
        "claude4_sonnet_mm": None,
        "grok3": "grok-3",
        "grok3_mini": "grok-3-mini",
        "grok4": "grok-4"
    },
    # https://api.together.ai/models

    "togetherai":{
        "qwen25_72b_instruct": "Qwen/Qwen2.5-72B-Instruct-Turbo",
        "qwen25_vl_72b_instruct": "Qwen/Qwen2.5-VL-72B-Instruct",
        "llama4_maverick": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
        "llama4_scout": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "llama32_vl_90b_instruct": "meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo",
        "claude37_normal": None,
        "claude37_thinking": None,
        "gemini20_flash": None,
        "gemini25_flash": None,
        "gemini25_pro": None,
        "gemini25_flash_thinking": None,
        "gpt41_normal": None,
        "gpt41_mini": None,
        "gpto4_mini_high": None,
        "gemini25_pro_mm": None,
        "gemini25_flash_mm": None,
        "gpt41_normal_mm": None,
        "gpto4_mini_high_mm": None,
        "llama4_maverick_mm": None,
        "llama4_scout_mm": None,
        "qwen3_235b_moe": "Qwen/Qwen3-235B-A22B-fp8-tput", 
        "qwen3_30b_moe": None,
        "claude4_opus": None,
        "claude4_sonnet": None,
        "claude4_sonnet_thinking": None,
        "claude4_opus_thinking": None,
        "claude4_sonnet_mm": None,
        "grok3": None,
        "grok3_mini": None,
        "grok4": None
    },
    # https://ai.google.dev/gemini-api/docs/models?hl=zh-cn#gemini-2.5-pro-preview-05-06

    "google":{
        "qwen25_72b_instruct": None,
        "qwen25_vl_72b_instruct": None,
        "llama4_maverick": None,
        "llama4_scout": None,
        "llama32_vl_90b_instruct": None,
        "claude37_normal": None,
        "claude37_thinking": None,
        "gemini20_flash": "gemini-2.0-flash",
        "gemini25_flash": "gemini-2.5-flash",
        "gemini25_pro": "gemini-2.5-pro",
        "gemini25_flash_thinking": None,
        "gpt41_normal": None,
        "gpt41_mini": None,
        "gpto4_mini_high": None,
        "gemini25_pro_mm": None,
        "gemini25_flash_mm": None,
        "gpt41_normal_mm": None,
        "gpto4_mini_high_mm": None,
        "llama4_maverick_mm": None,
        "llama4_scout_mm": None,
        "qwen3_235b_moe": None, 
        "qwen3_30b_moe": None,
        "claude4_opus": None,
        "claude4_sonnet": None,
        "claude4_sonnet_thinking": None,
        "claude4_opus_thinking": None,
        "claude4_sonnet_mm": None,
        "grok3": None,
        "grok3_mini": None,
        "grok4": None
    },
    # https://docs.anthropic.com/en/docs/about-claude/models/all-models
    
    "anthropic":{
        "qwen25_72b_instruct": None,
        "qwen25_vl_72b_instruct": None,
        "llama4_maverick": None,
        "llama4_scout": None,
        "llama32_vl_90b_instruct": None,
        "claude37_normal": "claude-3-7-sonnet-latest",
        "claude37_thinking": None,
        "gemini20_flash": None,
        "gemini25_flash": None,
        "gemini25_pro": None,
        "gemini25_flash_thinking": None,
        "gpt41_normal": None,
        "gpt41_mini": None,
        "gpto4_mini_high": None,
        "gemini25_pro_mm": None,
        "gemini25_flash_mm": None,
        "gpt41_normal_mm": None,
        "gpto4_mini_high_mm": None,
        "llama4_maverick_mm": None,
        "llama4_scout_mm": None,
        "qwen3_235b_moe": None, 
        "qwen3_30b_moe": None,
        "claude4_opus": "claude-opus-4-20250514",
        "claude4_sonnet": "claude-sonnet-4-20250514",
        "claude4_sonnet_thinking": None,
        "claude4_opus_thinking": None,
        "claude4_sonnet_mm": None,
        "grok3": None,
        "grok3_mini": None,
        "grok4": None
    },
    # https://platform.openai.com/docs/models
    
    "openai":{
        "qwen25_72b_instruct": None,
        "qwen25_vl_72b_instruct": None,
        "llama4_maverick": None,
        "llama4_scout": None,
        "llama32_vl_90b_instruct": None,
        "claude37_normal": None,
        "claude37_thinking": None,
        "gemini20_flash": None,
        "gemini25_flash": None,
        "gemini25_pro": None,
        "gemini25_flash_thinking": None,
        "gpt41_normal": "gpt-4.1-2025-04-14",
        "gpt41_mini": "gpt-4.1-mini-2025-04-14",
        "gpto4_mini_high": "o4-mini-2025-04-16",
        "gemini25_pro_mm": None,
        "gemini25_flash_mm": None,
        "gpt41_normal_mm": None,
        "gpto4_mini_high_mm": None,
        "llama4_maverick_mm": None,
        "llama4_scout_mm": None,
        "qwen3_235b_moe": None, 
        "qwen3_30b_moe": None,
        "claude4_opus": None,
        "claude4_sonnet": None,
        "claude4_sonnet_thinking": None,
        "claude4_opus_thinking": None,
        "claude4_sonnet_mm": None,
        "grok3": None,
        "grok3_mini": None,
        "grok4": None
    },

}

# è¾“å…¥tokenä»·æ ¼, ä¸è¾“å‡ºtokenä»·æ ¼.
# å¦‚æœä¸æ˜¯tuple, è€Œæ˜¯float, åˆ™i/oä»·æ ¼ä¸€è‡´.
source_price = {
    # https://openrouter.ai/models
    "openrouter": {
        "qwen/qwen-2.5-72b-instruct": (0.12*1e-6, 0.39*1e-6),
        "meta-llama/llama-4-maverick": (0.17*1e-6, 0.6*1e-6),
        "meta-llama/llama-4-scout": (0.08*1e-6, 0.3*1e-6),
        "anthropic/claude-3.7-sonnet": (3*1e-6, 15*1e-6),
        "anthropic/claude-3.7-sonnet:thinking": (3*1e-6, 15*1e-6),
        "google/gemini-2.5-flash": (0.15*1e-6, 0.6*1e-6),
        "google/gemini-2.5-pro": (1.25*1e-6, 10*1e-6),
        "google/gemini-2.5-flash-preview:thinking": (0.15*1e-6, 3.5*1e-6),
        "openai/gpt-4.1": (2*1e-6, 8*1e-6),
        "openai/gpt-4.1-mini": (0.4*1e-6, 1.6*1e-6),
        "openai/o4-mini-high": (1.1*1e-6, 4.4*1e-6),
        "anthropic/claude-opus-4": (15*1e-6, 75*1e-6),
        "anthropic/claude-sonnet-4": (3*1e-6, 15*1e-6),
        "x-ai/grok-3": (3*1e-6, 15*1e-6),
        "x-ai/grok-3-mini": (0.3*1e-6, 0.5*1e-6),
        "x-ai/grok-4": (3*1e-6, 15*1e-6)
    },
    # https://deepinfra.com/models

    "deepinfra": {
        "Qwen/Qwen2.5-72B-Instruct": (0.12*1e-6, 0.39*1e-6),
        "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8": (0.17*1e-6, 0.6*1e-6),
        "meta-llama/Llama-4-Scout-17B-16E-Instruct": (0.08*1e-6, 0.3*1e-6),
        "meta-llama/Llama-3.2-90B-Vision-Instruct": (0.35*1e-6, 0.4*1e-6),
        "anthropic/claude-3-7-sonnet-latest": (3.3*1e-6, 16.5*1e-6),
        "google/gemini-2.0-flash-001": (0.1*1e-6, 0.4*1e-6),
        "google/gemini-2.5-flash": (0.105*1e-6, 2.45*1e-6),
        "google/gemini-2.5-pro": (0.875*1e-6, 7*1e-6),
    },
    # https://api.deerapi.com/pricing

    "deerapi": {
        "Qwen2.5-72B-Instruct-128K": (4*1e-6, 12*1e-6),
        "llama-4-maverick": (0.6*1e-6, 1.8*1e-6), 
        # "llama-4-scout": (0.27*1e-6, 1.44*1e-6),  # ç§»é™¤ï¼šä¸å¯ç”¨
        "claude-3-7-sonnet-latest": (3*1e-6, 15*1e-6),
        "claude-3-7-sonnet-thinking": (3*1e-6, 15*1e-6),
        "gemini-2.0-flash": (0.1*1e-6, 0.4*1e-6),
        "gemini-2.5-flash": (0.15*1e-6, 3.5*1e-6),
        "gemini-2.5-pro-preview-06-05": (1.2*1e-6, 10*1e-6),
        "gpt-4.1": (2*1e-6, 8*1e-6),
        "gpt-4.1-mini": (0.4*1e-6, 1.6*1e-6),
        "qwen3-235b-a22b": (0.4*1e-6, 1.2*1e-6),
        "qwen3-30b-a3b": (2*1e-6, 6*1e-6),
        "claude-opus-4-20250514": (15*1e-6, 75*1e-6),
        "claude-sonnet-4-20250514": (3*1e-6, 15*1e-6),
        "claude-sonnet-4-20250514-thinking": (3*1e-6, 15*1e-6),
        "claude-opus-4-20250514-thinking": (15*1e-6, 75*1e-6),
        "grok-3": (3*1e-6, 9*1e-6),
        "grok-3-mini": (0.3*1e-6, 0.9*1e-6),
        "grok-4": (3*1e-6, 9*1e-6)
    },
    # https://api.together.ai/models

    "togetherai":{
        "Qwen/Qwen2.5-72B-Instruct-Turbo": (1.2*1e-6, 1.2*1e-6),
        "Qwen/Qwen2.5-VL-72B-Instruct": (1.98*1e-6, 8*1e-6),
        "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8": (0.27*1e-6, 0.85*1e-6),
        "meta-llama/Llama-4-Scout-17B-16E-Instruct": (0.18*1e-6, 0.59*1e-6),
        "meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo": (1.2*1e-6, 1.2*1e-6),
        "Qwen/Qwen3-235B-A22B-fp8-tput": (0.20*1e-6, 0.60*1e-6)
    },
    # https://ai.google.dev/gemini-api/docs/models?hl=zh-cn#gemini-2.5-pro-preview-05-06

    "google":{
        "gemini-2.0-flash": (0.1*1e-6, 0.4*1e-6),
        "gemini-2.5-flash": (0.15*1e-6, 1*1e-6),
        "gemini-2.5-pro": (1.25*1e-6, 10*1e-6),
    },
    # https://docs.anthropic.com/en/docs/about-claude/models/all-models
    
    "anthropic":{
        "claude-3-7-sonnet-latest": (3*1e-6, 15*1e-6),
        "claude-opus-4-20250514":  (15*1e-6, 75*1e-6),
        "claude-sonnet-4-20250514": (3*1e-6, 15*1e-6),
    },
    # https://platform.openai.com/docs/models
    
    "openai":{
        "gpt-4.1-2025-04-14": (2*1e-6, 8*1e-6),
        "gpt-4.1-mini-2025-04-14": (0.4*1e-6, 1.6*1e-6),
        "o4-mini-2025-04-16": (1.1*1e-6, 4.4*1e-6),
    },

}

source_max_ioLength = {
    # https://openrouter.ai/models

    "openrouter": {
        "qwen/qwen-2.5-72b-instruct": 33 * 1e3,
        "meta-llama/llama-4-maverick": 1050 * 1e3,
        "meta-llama/llama-4-scout": 1050 * 1e3,
        "anthropic/claude-3.7-sonnet": 200 * 1e3,
        "anthropic/claude-3.7-sonnet:thinking": 200 * 1e3,
        "google/gemini-2.5-flash": 1050 * 1e3,
        "google/gemini-2.5-pro": 1050 * 1e3,
        "google/gemini-2.5-flash-preview:thinking": 1050 * 1e3,
        "openai/gpt-4.1": 1050 * 1e3,
        "openai/gpt-4.1-mini": 1050 * 1e3,
        "openai/o4-mini-high": 200 * 1e3,
        "anthropic/claude-opus-4": 200*1e3,
        "anthropic/claude-sonnet-4": 200*1e3,
        "x-ai/grok-3": 131072,
        "x-ai/grok-3-mini": 131072,
        "x-ai/grok-4": 256*1e3,
    },
    # https://deepinfra.com/models

    "deepinfra": {
        "Qwen/Qwen2.5-72B-Instruct": 32 * 1e3,
        "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8": 1024 * 1e3,
        "meta-llama/Llama-4-Scout-17B-16E-Instruct": 320 * 1e3,
        "meta-llama/Llama-3.2-90B-Vision-Instruct": 32 * 1e3,
        "anthropic/claude-3-7-sonnet-latest": 195 * 1e3,
        "google/gemini-2.0-flash-001": 976 * 1e3,
        "google/gemini-2.5-flash": 976 * 1e3,
        "google/gemini-2.5-pro": 976 * 1e3,
    },
    # https://api.deerapi.com/pricing

    "deerapi": {
        "Qwen2.5-72B-Instruct-128K": None,
        "llama-4-maverick": None, 
        # "llama-4-scout": None,  # ç§»é™¤ï¼šä¸å¯ç”¨
        "claude-3-7-sonnet-latest": None,
        "claude-3-7-sonnet-thinking": None,
        "gemini-2.0-flash": None,
        "gemini-2.5-flash": None,
        "gemini-2.5-pro-preview-06-05": None,
        "gpt-4.1": None,
        "gpt-4.1-mini": None,
        "qwen3-235b-a22b": None,
        "qwen3-30b-a3b": None,
        "claude-opus-4-20250514": None,
        "claude-sonnet-4-20250514": None,
        "claude-sonnet-4-20250514-thinking": None,
        "claude-opus-4-20250514-thinking": None,
        "grok-3": 131072,
        "grok-3-mini": 131072,
        "grok-4": 256*1e3,
    },
    # https://api.together.ai/models

    "togetherai":{
        "Qwen/Qwen2.5-72B-Instruct-Turbo": None,
        "Qwen/Qwen2.5-VL-72B-Instruct": None,
        "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8": None,
        "meta-llama/Llama-4-Scout-17B-16E-Instruct": None,
        "meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo": None,
        "Qwen/Qwen3-235B-A22B-fp8-tput": None
    },
    # https://ai.google.dev/gemini-api/docs/models?hl=zh-cn#gemini-2.5-pro-preview-05-06

    "google":{
        "gemini-2.0-flash": 1048576,
        "gemini-2.5-flash": 1048576,
        "gemini-2.5-pro": 1048576,
    },
    # https://docs.anthropic.com/en/docs/about-claude/models/all-models
    
    "anthropic":{
        "claude-3-7-sonnet-latest": 200*1e3,
        "claude-opus-4-20250514": 200*1e3,
        "claude-sonnet-4-20250514": 200*1e3,
    },
    # https://platform.openai.com/docs/models
    
    "openai":{
        "gpt-4.1-2025-04-14": 1047576,
        "gpt-4.1-mini-2025-04-14": 1047576,
        "o4-mini-2025-04-16": 200000,
    },

}

def count_model_sources():
    """
    ç»Ÿè®¡æ¯ä¸ªæ¨¡å‹åœ¨å¤šå°‘ä¸ªæºï¼ˆä¾›åº”å•†ï¼‰ä¸­å¯ç”¨
    
    è€ƒè™‘å¤šæ¨¡æ€æ¨¡å‹å’Œéå¤šæ¨¡æ€æ¨¡å‹çš„æ˜ å°„å…³ç³»ï¼š
    - å¦‚æœå¤šæ¨¡æ€æ¨¡å‹å€¼ä¸ºNoneï¼Œåˆ™ä½¿ç”¨å¯¹åº”çš„éå¤šæ¨¡æ€æ¨¡å‹
    - ç»Ÿè®¡æ¯ä¸ªæ¨¡å‹å®é™…å¯ç”¨çš„æºæ•°é‡
    
    è¿”å›:
    - model_source_count: å­—å…¸ï¼Œæ ¼å¼ä¸º {model_name: available_source_count}
    """
    # è·å–æ‰€æœ‰æ¨¡å‹åˆ—è¡¨
    all_models = set()
    all_models.update(model_list_normal)
    all_models.update(model_list_thinking) 
    all_models.update(model_list_mm_normal)
    all_models.update(model_list_mm_thinking)
    
    model_source_count = {}
    
    # éå†æ¯ä¸ªæ¨¡å‹
    for model_name in sorted(all_models):
        available_sources = []
        
        # éå†æ¯ä¸ªæº
        for source_name in source_mapping:
            # è·å–åŸºç¡€æ¨¡å‹åï¼ˆå»æ‰_mmåç¼€ï¼‰
            base_model_name = model_name[:-3] if model_name.endswith("_mm") else model_name
            
            # æ£€æŸ¥æºæ˜¯å¦æ”¯æŒè¿™ä¸ªæ¨¡å‹
            if base_model_name in source_mapping[source_name]:
                source_model_name = source_mapping[source_name][base_model_name]
                
                # å¦‚æœå¤šæ¨¡æ€æ¨¡å‹çš„å€¼ä¸ºNoneï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„éå¤šæ¨¡æ€ç‰ˆæœ¬
                if model_name.endswith("_mm") and source_model_name is None:
                    # å¤šæ¨¡æ€æ¨¡å‹å€¼ä¸ºNoneï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹çš„æ˜ å°„
                    if base_model_name in source_mapping[source_name] and source_mapping[source_name][base_model_name] is not None:
                        available_sources.append(source_name)
                elif source_model_name is not None:
                    # ç›´æ¥å¯ç”¨
                    available_sources.append(source_name)
        
        model_source_count[model_name] = len(available_sources)
    
    return model_source_count

def print_model_source_statistics():
    """
    æ‰“å°æ¯ä¸ªæ¨¡å‹çš„æºå¯ç”¨æ€§ç»Ÿè®¡ä¿¡æ¯
    
    æŒ‰æ¨¡å‹ç±»å‹åˆ†ç»„æ˜¾ç¤ºï¼Œå¹¶æä¾›è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
    """
    model_counts = count_model_sources()
    
    print("=" * 80)
    print("ğŸ“Š æ¨¡å‹æºå¯ç”¨æ€§ç»Ÿè®¡æŠ¥å‘Š")
    print("=" * 80)
    
    # æŒ‰æ¨¡å‹ç±»å‹åˆ†ç»„
    model_groups = [
        ("éæ¨ç†ç±»å¤§è¯­è¨€æ¨¡å‹", model_list_normal),
        ("æ¨ç†ç±»å¤§è¯­è¨€æ¨¡å‹", model_list_thinking),
        ("éæ¨ç†ç±»å¤šæ¨¡æ€å¤§æ¨¡å‹", model_list_mm_normal), 
        ("æ¨ç†ç±»å¤šæ¨¡æ€å¤§æ¨¡å‹", model_list_mm_thinking)
    ]
    
    total_models = 0
    total_source_assignments = 0
    
    for group_name, model_list in model_groups:
        if not model_list:
            continue
            
        print(f"\nğŸ”¹ {group_name}")
        print("-" * 60)
        
        group_models = 0
        group_source_assignments = 0
        
        for model_name in model_list:
            source_count = model_counts.get(model_name, 0)
            group_models += 1
            group_source_assignments += source_count
            
            # æ ¹æ®å¯ç”¨æºæ•°é‡æ˜¾ç¤ºä¸åŒçš„çŠ¶æ€å›¾æ ‡
            if source_count == 0:
                status = "âŒ"
            elif source_count == 1:
                status = "âš ï¸ "
            elif source_count <= 3:
                status = "âœ…"
            else:
                status = "ğŸŒŸ"
            
            print(f"  {status} {model_name:<25} -> {source_count} ä¸ªæº")
        
        avg_sources = group_source_assignments / group_models if group_models > 0 else 0
        print(f"  ğŸ“ˆ å°è®¡: {group_models} ä¸ªæ¨¡å‹ï¼Œå¹³å‡ {avg_sources:.1f} ä¸ªæº/æ¨¡å‹")
        
        total_models += group_models
        total_source_assignments += group_source_assignments
    
    # æ€»ä½“ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("ğŸ“‹ æ€»ä½“ç»Ÿè®¡")
    print("=" * 80)
    
    avg_sources_overall = total_source_assignments / total_models if total_models > 0 else 0
    print(f"æ€»æ¨¡å‹æ•°: {total_models}")
    print(f"æ€»æºåˆ†é…æ•°: {total_source_assignments}")
    print(f"å¹³å‡æºæ•°/æ¨¡å‹: {avg_sources_overall:.2f}")
    
    # æºè¦†ç›–ç‡åˆ†æ
    print(f"\nğŸ” æºè¦†ç›–ç‡åˆ†æ:")
    coverage_stats = {}
    for count in model_counts.values():
        coverage_stats[count] = coverage_stats.get(count, 0) + 1
    
    for source_count in sorted(coverage_stats.keys()):
        model_count = coverage_stats[source_count]
        percentage = (model_count / total_models) * 100 if total_models > 0 else 0
        
        if source_count == 0:
            status = "âŒ ä¸å¯ç”¨"
        elif source_count == 1:
            status = "âš ï¸  å•ä¸€æº"
        elif source_count <= 3:
            status = "âœ… å¤šæºæ”¯æŒ"
        else:
            status = "ğŸŒŸ é«˜å¯ç”¨æ€§"
        
        print(f"  {status}: {model_count} ä¸ªæ¨¡å‹ ({percentage:.1f}%) åœ¨ {source_count} ä¸ªæºä¸­å¯ç”¨")
    
    # å¤šæ¨¡æ€æ˜ å°„åˆ†æ
    print(f"\nğŸ”„ å¤šæ¨¡æ€æ¨¡å‹æ˜ å°„åˆ†æ:")
    mm_models = [m for m in model_counts.keys() if m.endswith("_mm")]
    mapped_count = 0
    
    for mm_model in mm_models:
        base_model = mm_model[:-3]
        mm_sources = model_counts[mm_model]
        base_sources = model_counts.get(base_model, 0)
        
        if mm_sources > 0 and base_sources > 0:
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†åŸºç¡€æ¨¡å‹çš„æ˜ å°„
            using_base_mapping = False
            for source_name in source_mapping:
                if base_model in source_mapping[source_name]:
                    if source_mapping[source_name][base_model] is not None:
                        using_base_mapping = True
                        break
            
            if using_base_mapping:
                mapped_count += 1
    
    print(f"  ğŸ“Š å¤šæ¨¡æ€æ¨¡å‹æ€»æ•°: {len(mm_models)}")
    print(f"  ğŸ”— ä½¿ç”¨åŸºç¡€æ¨¡å‹æ˜ å°„: {mapped_count} ä¸ª")
    print(f"  ğŸ“ˆ æ˜ å°„ä½¿ç”¨ç‡: {(mapped_count/len(mm_models)*100):.1f}%" if mm_models else "0%")
    
    print("\n" + "=" * 80)
    print("âœ… ç»Ÿè®¡å®Œæˆ")
    print("=" * 80)

def validate_mappings():
    """
    éªŒè¯æ‰€æœ‰çš„æ¨¡å‹åˆ—è¡¨å’Œæ˜ å°„æ˜¯å¦ä¸€è‡´å’Œäº’ç›¸åŒ…å«
    
    æ£€æŸ¥:
    1. æ‰€æœ‰æ¨¡å‹åˆ—è¡¨ä¸­çš„æ¨¡å‹éƒ½åœ¨source_mappingä¸­
    2. æ‰€æœ‰é…ç½®ä¸­çš„æºæ˜¯å¦ä¸€è‡´(source_config, source_ranking, source_mapping, source_price, source_max_ioLength)
    3. å¸¦æœ‰"_mm"åç¼€ä¸”å€¼ä¸ºNoneçš„æ¨¡å‹åº”è¯¥æœ‰ç›¸åº”çš„éå¤šæ¨¡æ€ç‰ˆæœ¬
    4. source_priceå’Œsource_max_ioLengthçš„å­—æ®µåº”å®Œå…¨ä¸€è‡´
    5. source_priceå’Œsource_max_ioLengthä¸­çš„æ¨¡å‹åº”å¯¹åº”source_mappingä¸­å€¼ä¸ä¸ºNoneçš„æ¨¡å‹
    6. å¥åº·æ£€æµ‹å±è”½æ¸…å•ä¸­çš„æ¨¡å‹å¿…é¡»å­˜åœ¨äºæ¨¡å‹åˆ—è¡¨ä¸­
    
    è¿”å›:
    - errors: å‘ç°çš„é”™è¯¯åˆ—è¡¨
    - warnings: å‘ç°çš„è­¦å‘Šåˆ—è¡¨
    """
    errors = []
    warnings = []
    
    # 1. è·å–æ‰€æœ‰æ¨¡å‹åˆ—è¡¨ä¸­çš„æ¨¡å‹
    all_models = set()
    all_models.update(model_list_normal)
    all_models.update(model_list_thinking)
    all_models.update(model_list_mm_normal)
    all_models.update(model_list_mm_thinking)
    
    # 2. æ£€æŸ¥æºé…ç½®çš„ä¸€è‡´æ€§
    source_config_keys = set(source_config.keys())
    source_ranking_set = set(source_ranking)
    source_mapping_keys = set(source_mapping.keys())
    source_price_keys = set(source_price.keys())
    source_max_ioLength_keys = set(source_max_ioLength.keys())
    
    # æ£€æŸ¥æ‰€æœ‰æºé…ç½®ä¸­çš„æºæ˜¯å¦ä¸€è‡´
    all_source_sets = [
        ("source_config", source_config_keys),
        ("source_ranking", source_ranking_set),
        ("source_mapping", source_mapping_keys),
        ("source_price", source_price_keys),
        ("source_max_ioLength", source_max_ioLength_keys)
    ]
    
    # æ¯”è¾ƒæ‰€æœ‰æºé›†åˆçš„ä¸€è‡´æ€§
    for i, (name1, set1) in enumerate(all_source_sets):
        for name2, set2 in all_source_sets[i+1:]:
            if set1 != set2:
                missing = set2 - set1
                extra = set1 - set2
                if missing:
                    errors.append(f"æºé…ç½®ä¸ä¸€è‡´: {name1}ä¸­ç¼ºå°‘{name2}ä¸­çš„æº: {missing}")
                if extra:
                    errors.append(f"æºé…ç½®ä¸ä¸€è‡´: {name1}ä¸­æœ‰é¢å¤–çš„{name2}ä¸­æ²¡æœ‰çš„æº: {extra}")
    
    # 3. æ£€æŸ¥æ¯ä¸ªæºä¸‹æ˜¯å¦æœ‰æ‰€æœ‰çš„æ¨¡å‹
    for source, models_map in source_mapping.items():
        model_keys = set(models_map.keys())
        
        # æ”¶é›†æ‰€æœ‰åˆ—è¡¨ä¸­çš„æ¨¡å‹ä½†åœ¨å½“å‰æºæ˜ å°„ä¸­ç¼ºå¤±çš„
        missing_models = all_models - model_keys
        if missing_models:
            errors.append(f"æº '{source}' ç¼ºå°‘æ¨¡å‹: {missing_models}")
        
        # æ”¶é›†å½“å‰æºæ˜ å°„ä¸­æœ‰ä½†ä¸åœ¨ä»»ä½•æ¨¡å‹åˆ—è¡¨ä¸­çš„
        extra_models = model_keys - all_models
        if extra_models:
            warnings.append(f"æº '{source}' æœ‰é¢å¤–çš„æ¨¡å‹: {extra_models}")
    
    # 4. æ£€æŸ¥_mmæ¨¡å‹çš„ç‰¹æ®Šé€»è¾‘
    for source, models_map in source_mapping.items():
        for model_name, model_value in models_map.items():
            if model_name.endswith("_mm") and model_value is None:
                # å»æ‰_mmåç¼€è·å–åŸºç¡€æ¨¡å‹å
                base_model_name = model_name[:-3]
                
                # æ£€æŸ¥åŸºç¡€æ¨¡å‹æ˜¯å¦å­˜åœ¨äºè¯¥æº
                if base_model_name not in models_map:
                    errors.append(f"æº '{source}' ä¸­æ¨¡å‹ '{model_name}' å€¼ä¸ºNoneï¼Œä½†æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„åŸºç¡€æ¨¡å‹ '{base_model_name}'")
    
    # 5. æ£€æŸ¥source_priceå’Œsource_max_ioLengthçš„æ¨¡å‹ä¸source_mappingçš„ä¸€è‡´æ€§
    for source in source_mapping:
        if source not in source_price or source not in source_max_ioLength:
            continue  # å·²åœ¨å‰é¢çš„æ£€æŸ¥ä¸­æŠ¥å‘Šè¿‡é”™è¯¯
        
        # è·å–å½“å‰æºä¸­æœ‰æ•ˆçš„æ¨¡å‹è·¯å¾„ï¼ˆä¸ä¸ºNoneçš„å€¼ï¼‰
        valid_model_paths = {v for k, v in source_mapping[source].items() if v is not None}
        price_model_paths = set(source_price[source].keys())
        iolength_model_paths = set(source_max_ioLength[source].keys())
        
        # æ£€æŸ¥source_priceå’Œsource_max_ioLengthä¸­çš„æ¨¡å‹æ˜¯å¦ä¸€è‡´
        if price_model_paths != iolength_model_paths:
            missing_in_price = iolength_model_paths - price_model_paths
            missing_in_iolength = price_model_paths - iolength_model_paths
            
            if missing_in_price:
                errors.append(f"æº '{source}' ä¸­çš„æ¨¡å‹åœ¨source_max_ioLengthä¸­å­˜åœ¨ä½†åœ¨source_priceä¸­ç¼ºå¤±: {missing_in_price}")
            if missing_in_iolength:
                errors.append(f"æº '{source}' ä¸­çš„æ¨¡å‹åœ¨source_priceä¸­å­˜åœ¨ä½†åœ¨source_max_ioLengthä¸­ç¼ºå¤±: {missing_in_iolength}")
                
        # éªŒè¯æ¨¡å‹åœ¨source_mappingä¸­å­˜åœ¨ï¼Œä¸”å¯¹åº”çš„å€¼ä¸ä¸ºNone
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹åœ¨source_mappingä¸­(ä¸ä¸ºNone)ä½†ä¸åœ¨priceå’Œiolengthä¸­
        missing_price_models = valid_model_paths - price_model_paths
        if missing_price_models:
            errors.append(f"æº '{source}' ä¸­çš„æ¨¡å‹æ˜ å°„å­˜åœ¨ä½†ç¼ºå°‘ä»·æ ¼å®šä¹‰: {missing_price_models}")
            
        missing_iolength_models = valid_model_paths - iolength_model_paths
        if missing_iolength_models:
            errors.append(f"æº '{source}' ä¸­çš„æ¨¡å‹æ˜ å°„å­˜åœ¨ä½†ç¼ºå°‘æœ€å¤§IOé•¿åº¦å®šä¹‰: {missing_iolength_models}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹åœ¨priceå’Œiolengthä¸­ä½†ä¸åœ¨source_mappingä¸­(æˆ–åœ¨source_mappingä¸­ä½†ä¸ºNone)
        extra_price_models = price_model_paths - valid_model_paths
        if extra_price_models:
            errors.append(f"æº '{source}' ä¸­å®šä¹‰äº†ä¸å­˜åœ¨äºæ¨¡å‹æ˜ å°„çš„ä»·æ ¼: {extra_price_models}")
            
        extra_iolength_models = iolength_model_paths - valid_model_paths
        if extra_iolength_models:
            errors.append(f"æº '{source}' ä¸­å®šä¹‰äº†ä¸å­˜åœ¨äºæ¨¡å‹æ˜ å°„çš„æœ€å¤§IOé•¿åº¦: {extra_iolength_models}")
        
        # æ£€æŸ¥æ ¼å¼
        # ä»·æ ¼æ ¼å¼æ£€æŸ¥
        for model_path, price in source_price[source].items():
            if price is not None and not isinstance(price, (float, tuple)):
                errors.append(f"æº '{source}' ä¸­æ¨¡å‹ '{model_path}' çš„ä»·æ ¼æ ¼å¼é”™è¯¯: {price}ï¼Œåº”ä¸ºNone, floatæˆ–tuple")
            elif isinstance(price, tuple) and (len(price) != 2 or not all(isinstance(p, float) or p is None for p in price)):
                errors.append(f"æº '{source}' ä¸­æ¨¡å‹ '{model_path}' çš„ä»·æ ¼å…ƒç»„æ ¼å¼é”™è¯¯: {price}ï¼Œåº”ä¸º(input_price, output_price)")
        
        # æœ€å¤§IOé•¿åº¦æ ¼å¼æ£€æŸ¥
        for model_path, iolength in source_max_ioLength[source].items():
            if iolength is not None and not isinstance(iolength, (int, float)):
                errors.append(f"æº '{source}' ä¸­æ¨¡å‹ '{model_path}' çš„æœ€å¤§IOé•¿åº¦æ ¼å¼é”™è¯¯: {iolength}ï¼Œåº”ä¸ºNone, intæˆ–float")
    
    # 6. æ£€æŸ¥å¥åº·æ£€æµ‹å±è”½æ¸…å•
    invalid_blacklisted_models = set(health_check_blacklist) - all_models
    if invalid_blacklisted_models:
        errors.append(f"å¥åº·æ£€æµ‹å±è”½æ¸…å•ä¸­åŒ…å«ä¸å­˜åœ¨çš„æ¨¡å‹: {invalid_blacklisted_models}")
    
    # éªŒè¯å±è”½æ¸…å•ä¸­çš„æ¨¡å‹è‡³å°‘åœ¨ä¸€ä¸ªæºä¸Šå¯ç”¨
    unavailable_blacklisted_models = []
    for model in health_check_blacklist:
        if model in all_models:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨è‡³å°‘ä¸€ä¸ªæºä¸Šå¯ç”¨
            available_in_any_source = False
            for source, models_map in source_mapping.items():
                base_model_name = model[:-3] if model.endswith("_mm") else model
                if base_model_name in models_map and models_map[base_model_name] is not None:
                    available_in_any_source = True
                    break
            if not available_in_any_source:
                unavailable_blacklisted_models.append(model)
    
    if unavailable_blacklisted_models:
        warnings.append(f"å¥åº·æ£€æµ‹å±è”½æ¸…å•ä¸­çš„æ¨¡å‹åœ¨ä»»ä½•æºä¸Šéƒ½ä¸å¯ç”¨: {unavailable_blacklisted_models}")
    
    return errors, warnings

if __name__ == "__main__":
    print("ğŸ”§ è¿è¡Œé…ç½®éªŒè¯...")
    errors, warnings = validate_mappings()
    print_model_source_statistics()

