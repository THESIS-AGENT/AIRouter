from fastapi import FastAPI, Depends, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy.orm import Session
from sqlalchemy import and_, func
from typing import Dict, List, Tuple, Set
import logging
from datetime import datetime, timedelta
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
from collections import defaultdict, Counter
import asyncio

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from .models import get_db, ApiKeyUsage, create_tables
from .schemas import ApiKeyRequest, ApiKeyUsageCreate, ApiKeyResponse

# å¯¼å…¥é…ç½®
import sys
import os
# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥é…ç½®
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ä»æ­£ç¡®çš„é…ç½®æ–‡ä»¶å¯¼å…¥æ‰€æœ‰é…ç½®
from ew_config.api_keys import pool_mapping
from ew_config.source import *

# é…ç½®å‚æ•°
# ä¸å¯ä»¥å¿å—æœ€è¿‘{TOLERANCE_TIMER_SPAN}åˆ†é’Ÿå†…å­˜åœ¨ä»»æ„ä¸€æ¬¡é”™è¯¯è®°å½•çš„api_key.
TOLERANCE_TIMER_SPAN = int(os.environ.get("TOLERANCE_TIMER_SPAN", 15))
# å®šæ—¶ä»»åŠ¡é—´éš”ï¼šæ¯å¤šå°‘åˆ†é’ŸæŸ¥è¯¢ä¸€æ¬¡å¤±è´¥çš„APIå¯†é’¥ï¼Œé»˜è®¤1åˆ†é’Ÿ
FAILED_KEY_CHECK_INTERVAL = int(os.environ.get("FAILED_KEY_CHECK_INTERVAL", 1))

# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    level=logging.INFO,  # æ”¹ä¸ºINFOçº§åˆ«ä»¥æ˜¾ç¤ºæ±‡æ€»æŠ¥å‘Š
    format='%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s'
)
logger = logging.getLogger(__name__)

# å‡å°‘FastAPIå’ŒSQLAlchemyçš„æ—¥å¿—å™ªéŸ³
logging.getLogger("uvicorn.access").setLevel(logging.ERROR)
logging.getLogger("sqlalchemy.engine").setLevel(logging.ERROR)
logging.getLogger("uvicorn").setLevel(logging.ERROR)
logging.getLogger("apscheduler").setLevel(logging.WARNING)

app = FastAPI(
    title="API Key Manager",
    description="Service for managing API keys with load balancing and failure tracking",
    version="2.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== å…¨å±€ç¼“å­˜å’ŒçŠ¶æ€ç®¡ç† ====================

# å¤±è´¥çš„APIå¯†é’¥ç¼“å­˜ï¼š{source_name: {api_key1, api_key2, ...}}
failed_keys_cache: Dict[str, Set[str]] = defaultdict(set)

# APIå¯†é’¥å¤±è´¥æ¬¡æ•°ç¼“å­˜ï¼š{source_name: {api_key: failure_count}}
key_failure_count_cache: Dict[str, Dict[str, int]] = defaultdict(dict)

# å¯†é’¥ä½¿ç”¨ç´¢å¼•ç¼“å­˜ï¼Œç”¨äºè´Ÿè½½å‡è¡¡
key_usage_cache: Dict[str, Dict[str, int]] = {}
global_index_cache: Dict[str, int] = {}

# ç»Ÿè®¡ä¿¡æ¯ç¼“å­˜
stats_cache: Dict[str, Dict] = {}

# æœ€åæ›´æ–°æ—¶é—´
last_update_time: datetime = datetime.now()

# ==================== å®šæ—¶ä»»åŠ¡å‡½æ•° ====================

def update_failed_keys_cache():
    """å®šæ—¶ä»»åŠ¡ï¼šæ›´æ–°å¤±è´¥çš„APIå¯†é’¥ç¼“å­˜"""
    global last_update_time
    start_time = datetime.now()
    logger.info("ğŸ”„ å¼€å§‹æ›´æ–°å¤±è´¥APIå¯†é’¥ç¼“å­˜...")
    
    try:
        # è·å–æ•°æ®åº“ä¼šè¯
        db = next(get_db())
        
        # è®¡ç®—æ—¶é—´çª—å£
        cutoff_time = datetime.now() - timedelta(minutes=TOLERANCE_TIMER_SPAN)
        
        # é‡ç½®ç¼“å­˜
        failed_keys_cache.clear()
        key_failure_count_cache.clear()
        stats_cache.clear()
        
        # æ‰¹é‡æŸ¥è¯¢æ‰€æœ‰å¤±è´¥çš„APIå¯†é’¥ - ä½¿ç”¨ä¼˜åŒ–çš„ç´¢å¼•æŸ¥è¯¢
        failed_keys_query = db.query(
            ApiKeyUsage.source_name,
            ApiKeyUsage.api_key,
            func.count(ApiKeyUsage.request_id).label('failure_count'),
            func.max(ApiKeyUsage.finish_time).label('last_failure')
        ).filter(
            and_(
                ApiKeyUsage.status == False,
                ApiKeyUsage.finish_time >= cutoff_time
            )
        ).group_by(
            ApiKeyUsage.source_name,
            ApiKeyUsage.api_key
        ).all()
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_failed_keys = 0
        source_stats = defaultdict(lambda: {
            'failed_keys': 0,
            'total_failures': 0,
            'models_affected': set(),
            'last_failure': None
        })
        
        # å¤„ç†æŸ¥è¯¢ç»“æœ
        for row in failed_keys_query:
            source_name = row.source_name
            api_key = row.api_key
            failure_count = row.failure_count
            last_failure = row.last_failure
            
            # æ·»åŠ åˆ°å¤±è´¥ç¼“å­˜
            failed_keys_cache[source_name].add(api_key)
            # è®°å½•å¤±è´¥æ¬¡æ•°
            key_failure_count_cache[source_name][api_key] = failure_count
            total_failed_keys += 1
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            source_stats[source_name]['failed_keys'] += 1
            source_stats[source_name]['total_failures'] += failure_count
            if source_stats[source_name]['last_failure'] is None or last_failure > source_stats[source_name]['last_failure']:
                source_stats[source_name]['last_failure'] = last_failure
        
        # æŸ¥è¯¢å—å½±å“çš„æ¨¡å‹ç»Ÿè®¡
        model_stats_query = db.query(
            ApiKeyUsage.source_name,
            ApiKeyUsage.model_name,
            func.count(ApiKeyUsage.request_id).label('failure_count')
        ).filter(
            and_(
                ApiKeyUsage.status == False,
                ApiKeyUsage.finish_time >= cutoff_time
            )
        ).group_by(
            ApiKeyUsage.source_name,
            ApiKeyUsage.model_name
        ).all()
        
        for row in model_stats_query:
            source_stats[row.source_name]['models_affected'].add(row.model_name)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_cache.update(source_stats)
        
        # å…³é—­æ•°æ®åº“ä¼šè¯
        db.close()
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        execution_time = (datetime.now() - start_time).total_seconds()
        last_update_time = datetime.now()
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        _generate_failed_keys_summary_report(total_failed_keys, source_stats, execution_time)
        
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°å¤±è´¥APIå¯†é’¥ç¼“å­˜æ—¶å‡ºé”™: {str(e)}")
        
def _generate_failed_keys_summary_report(total_failed_keys: int, source_stats: dict, execution_time: float):
    """ç”Ÿæˆå¤±è´¥å¯†é’¥çš„æ±‡æ€»æŠ¥å‘Š"""
    logger.info("ğŸ“Š ==================== APIå¯†é’¥å¤±è´¥æƒ…å†µæ±‡æ€»æŠ¥å‘Š ====================")
    logger.info(f"ğŸ“Š æŸ¥è¯¢æ‰§è¡Œæ—¶é—´: {execution_time:.2f}s")
    logger.info(f"ğŸ“Š æ—¶é—´çª—å£: æœ€è¿‘{TOLERANCE_TIMER_SPAN}åˆ†é’Ÿ")
    logger.info(f"ğŸ“Š å¤±è´¥å¯†é’¥æ€»æ•°: {total_failed_keys}")
    
    if not source_stats:
        logger.info("ğŸ“Š âœ… æ‰€æœ‰APIå¯†é’¥è¿è¡Œæ­£å¸¸ï¼Œæ— å¤±è´¥è®°å½•")
        logger.info("ğŸ“Š ===============================================================")
        return
    
    # æŒ‰å¤±è´¥å¯†é’¥æ•°é‡æ’åº
    sorted_sources = sorted(source_stats.items(), key=lambda x: x[1]['failed_keys'], reverse=True)
    
    for source_name, stats in sorted_sources:
        logger.info(f"ğŸ“Š æº [{source_name}]:")
        logger.info(f"ğŸ“Š   - å¤±è´¥å¯†é’¥æ•°: {stats['failed_keys']}")
        logger.info(f"ğŸ“Š   - å¤±è´¥æ¬¡æ•°: {stats['total_failures']}")
        logger.info(f"ğŸ“Š   - å—å½±å“æ¨¡å‹: {len(stats['models_affected'])} ä¸ª")
        if stats['models_affected']:
            models_list = ', '.join(list(stats['models_affected'])[:5])  # åªæ˜¾ç¤ºå‰5ä¸ª
            if len(stats['models_affected']) > 5:
                models_list += f"... (å…±{len(stats['models_affected'])}ä¸ª)"
            logger.info(f"ğŸ“Š   - æ¨¡å‹åˆ—è¡¨: {models_list}")
        if stats['last_failure']:
            logger.info(f"ğŸ“Š   - æœ€åå¤±è´¥: {stats['last_failure'].strftime('%H:%M:%S')}")
    
    logger.info("ğŸ“Š ===============================================================")

# ==================== è°ƒåº¦å™¨è®¾ç½® ====================

scheduler = BackgroundScheduler()

@app.on_event("startup")
async def startup_event():
    """åœ¨å¯åŠ¨æ—¶åˆå§‹åŒ–æ•°æ®åº“è¡¨å’Œå®šæ—¶ä»»åŠ¡"""
    try:
        logger.info("ğŸš€ APIå¯†é’¥ç®¡ç†æœåŠ¡æ­£åœ¨å¯åŠ¨...")
        
        # æ£€æŸ¥å¹¶åˆå§‹åŒ–æ•°æ®åº“è¡¨ï¼ˆä»…åœ¨å¿…è¦æ—¶ï¼‰
        logger.info("ğŸ”§ æ£€æŸ¥æ•°æ®åº“è¡¨çŠ¶æ€...")
        create_tables()
        logger.info("ğŸ”§ æ•°æ®åº“è¡¨æ£€æŸ¥å®Œæˆ")
        
        # åˆå§‹åŒ–ç¼“å­˜
        logger.info("ğŸ”§ åˆå§‹åŒ–ç¼“å­˜...")
        # åªåˆå§‹åŒ–æœ‰APIå¯†é’¥çš„æº
        for source_name in pool_mapping.keys():
            if source_name not in key_usage_cache:
                key_usage_cache[source_name] = {}
            if source_name not in global_index_cache:
                global_index_cache[source_name] = -1
        
        logger.info(f"ğŸ”§ åˆå§‹åŒ–äº† {len(pool_mapping)} ä¸ªæºçš„ç¼“å­˜: {', '.join(pool_mapping.keys())}")
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡å¤±è´¥å¯†é’¥æ›´æ–°
        logger.info("ğŸ”§ æ‰§è¡Œåˆå§‹å¤±è´¥å¯†é’¥æ£€æŸ¥...")
        update_failed_keys_cache()
        
        # å¯åŠ¨å®šæ—¶ä»»åŠ¡
        scheduler.add_job(
            update_failed_keys_cache,
            trigger=IntervalTrigger(minutes=FAILED_KEY_CHECK_INTERVAL),
            id="failed_keys_check_job",
            name=f"Update failed API keys cache every {FAILED_KEY_CHECK_INTERVAL} minutes",
            replace_existing=True,
            max_instances=1,
            misfire_grace_time=300
        )
        
        scheduler.start()
        logger.info(f"â° å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ¯{FAILED_KEY_CHECK_INTERVAL}åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡å¤±è´¥çš„APIå¯†é’¥")
        logger.info("âœ… APIå¯†é’¥ç®¡ç†æœåŠ¡å¯åŠ¨å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–APIå¯†é’¥ç®¡ç†æœåŠ¡æ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail="æœåŠ¡åˆå§‹åŒ–å¤±è´¥")

@app.on_event("shutdown")
async def shutdown_event():
    """å…³é—­å®šæ—¶ä»»åŠ¡"""
    logger.info("ğŸ›‘ APIå¯†é’¥ç®¡ç†æœåŠ¡æ­£åœ¨å…³é—­...")
    scheduler.shutdown()
    logger.info("âœ… å®šæ—¶ä»»åŠ¡å·²å…³é—­")

# ==================== ä¸­é—´ä»¶å’Œå¥åº·æ£€æŸ¥ ====================

@app.middleware("http")
async def log_requests(request: Request, call_next):
    """è®°å½•æ‰€æœ‰HTTPè¯·æ±‚"""
    if request.url.path not in ["/health", "/check_healthy"]:
        logger.debug(f"æ”¶åˆ°è¯·æ±‚: {request.method} {request.url}")
    response = await call_next(request)
    if response.status_code >= 400:
        logger.warning(f"é”™è¯¯å“åº”: {response.status_code} for {request.method} {request.url}")
    elif request.url.path not in ["/health", "/check_healthy"]:
        logger.debug(f"å‘é€å“åº”: {response.status_code}")
    return response

@app.get("/health", response_model=Dict[str, str])
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy"}

@app.get("/check_healthy", response_model=Dict[str, str])
async def check_healthy():
    """ä¸“é—¨ç”¨äºDockerå¥åº·æ£€æŸ¥çš„ç«¯ç‚¹"""
    try:
        return {"status": "healthy", "timestamp": datetime.now().isoformat()}
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")

# ==================== æ ¸å¿ƒAPIç«¯ç‚¹ ====================

@app.post("/get_apikey", response_model=ApiKeyResponse)
async def get_api_key(request: ApiKeyRequest):
    """
    è·å–ç»™å®šæºçš„æœ€ä½³APIå¯†é’¥ã€‚
    ä½¿ç”¨ç¼“å­˜çš„å¤±è´¥å¯†é’¥åˆ—è¡¨ï¼Œå®ç°é«˜æ€§èƒ½çš„å¯†é’¥é€‰æ‹©ã€‚
    """
    logger.debug(f"è¯·æ±‚æº '{request.source_name}' çš„APIå¯†é’¥")
    source_name = request.source_name
    
    # æ£€æŸ¥æºæ˜¯å¦å­˜åœ¨
    if source_name not in pool_mapping:
        available_sources = list(pool_mapping.keys())
        logger.warning(f"è¯·æ±‚ä¸å­˜åœ¨çš„æº: '{source_name}' (å¯ç”¨æº: {', '.join(available_sources)})")
        raise HTTPException(status_code=404, detail=f"æ‰¾ä¸åˆ°æº '{source_name}'")
    
    # è·å–APIå¯†é’¥æ± 
    api_keys_pool = pool_mapping[source_name]
    
    # åˆå§‹åŒ–ç¼“å­˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if source_name not in key_usage_cache:
        key_usage_cache[source_name] = {}
    if source_name not in global_index_cache:
        global_index_cache[source_name] = -1
    
    # æ”¶é›†æ‰€æœ‰å¯ç”¨çš„APIå¯†é’¥
    all_keys: List[Tuple[str, int, str]] = []
    for user_name in api_keys_pool:
        if user_name not in key_usage_cache[source_name]:
            key_usage_cache[source_name][user_name] = -1
            
        for i, element in enumerate(api_keys_pool[user_name]):
            all_keys.append((user_name, i, element["api_key"]))
    
    if not all_keys:
        logger.error(f"æº '{source_name}' æ²¡æœ‰å¯ç”¨çš„APIå¯†é’¥")
        raise HTTPException(status_code=404, detail=f"æº '{source_name}' æ²¡æœ‰å¯ç”¨çš„APIå¯†é’¥")
    
    # ä½¿ç”¨ç¼“å­˜çš„å¤±è´¥å¯†é’¥åˆ—è¡¨ï¼ˆæ— éœ€æ•°æ®åº“æŸ¥è¯¢ï¼‰
    failing_keys = failed_keys_cache.get(source_name, set())
    
    # é¦–å…ˆå°è¯•ä½¿ç”¨æ­£å¸¸å·¥ä½œçš„å¯†é’¥
    working_keys = [(idx, user, key_idx, api_key) for idx, (user, key_idx, api_key) in enumerate(all_keys) if api_key not in failing_keys]
    
    if working_keys:
        # ä½¿ç”¨è½®è¯¢æ–¹å¼é€‰æ‹©æ­£å¸¸å·¥ä½œçš„å¯†é’¥
        current_global_index = global_index_cache[source_name]
        next_key_idx = (current_global_index + 1) % len(working_keys)
        global_index_cache[source_name] = next_key_idx
        
        _, user_name, key_index, api_key = working_keys[next_key_idx]
        key_usage_cache[source_name][user_name] = key_index
        
        logger.debug(f"è¿”å›APIå¯†é’¥ï¼ˆå¥åº·è½®è¯¢ï¼‰: {api_key[:8]}...")
        return ApiKeyResponse(api_key=api_key)
    
    # å¦‚æœæ‰€æœ‰å¯†é’¥éƒ½æœ‰å¤±è´¥è®°å½•ï¼Œé€‰æ‹©å¤±è´¥æ¬¡æ•°æœ€å°‘çš„å¯†é’¥
    cutoff_time = datetime.now() - timedelta(minutes=TOLERANCE_TIMER_SPAN)
    failure_counts = key_failure_count_cache.get(source_name, {})
    
    logger.warning(f"ğŸ” æº '{source_name}' æ‰€æœ‰APIå¯†é’¥éƒ½æœ‰å¤±è´¥è®°å½• (æ—¶é—´çª—å£: æœ€è¿‘{TOLERANCE_TIMER_SPAN}åˆ†é’Ÿ)ï¼Œé€‰æ‹©å¤±è´¥æ¬¡æ•°æœ€å°‘çš„å¯†é’¥")
    
    # è®¡ç®—æ¯ä¸ªå¯†é’¥çš„å¤±è´¥æ¬¡æ•°ï¼Œæ²¡æœ‰è®°å½•çš„è§†ä¸º0æ¬¡å¤±è´¥
    key_with_failures = []
    for user_name, key_index, api_key in all_keys:
        failure_count = failure_counts.get(api_key, 0)
        key_with_failures.append((failure_count, user_name, key_index, api_key))
    
    # æŒ‰å¤±è´¥æ¬¡æ•°æ’åºï¼Œé€‰æ‹©å¤±è´¥æ¬¡æ•°æœ€å°‘çš„
    key_with_failures.sort(key=lambda x: x[0])
    failure_count, user_name, key_index, api_key = key_with_failures[0]
    
    key_usage_cache[source_name][user_name] = key_index
    
    logger.info(f"ğŸ¯ é€‰æ‹©å¤±è´¥æ¬¡æ•°æœ€å°‘çš„APIå¯†é’¥: {api_key[:8]}... (å¤±è´¥æ¬¡æ•°: {failure_count})")
    return ApiKeyResponse(api_key=api_key)

@app.post("/notice_apikey", status_code=201)
async def notice_api_key(usage: ApiKeyUsageCreate, db: Session = Depends(get_db)):
    """
    è®°å½•APIå¯†é’¥ä½¿ç”¨ä¿¡æ¯ã€‚
    """
    # è®°å½•ä½¿ç”¨æƒ…å†µ
    if not usage.status:
        logger.warning(f"APIå¯†é’¥ä½¿ç”¨å¤±è´¥: {usage.api_key[:8]}... æ¨¡å‹: {usage.model_name} æº: {usage.source_name}")
    else:
        logger.debug(f"è®°å½•APIå¯†é’¥ä½¿ç”¨æƒ…å†µ: {usage.api_key[:8]}... çŠ¶æ€: {usage.status}")
    
    # åˆ›å»ºæ•°æ®åº“è®°å½•
    db_usage = ApiKeyUsage(
        request_id=usage.request_id,
        api_key=usage.api_key,
        model_name=usage.model_name,
        source_name=usage.source_name,
        prompt_tokens=usage.prompt_tokens,
        completion_tokens=usage.completion_tokens,
        create_time=usage.create_time,
        finish_time=usage.finish_time,
        execution_time=usage.execution_time,
        status=usage.status,
        remark=usage.remark or ""
    )
    
    try:
        db.add(db_usage)
        db.commit()
        db.refresh(db_usage)
        logger.debug("APIå¯†é’¥ä½¿ç”¨æƒ…å†µå·²æˆåŠŸè®°å½•åˆ°æ•°æ®åº“")
    except Exception as e:
        db.rollback()
        logger.error(f"è®°å½•APIå¯†é’¥ä½¿ç”¨æƒ…å†µåˆ°æ•°æ®åº“æ—¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail="è®°å½•åˆ°æ•°æ®åº“å¤±è´¥")
    
    return {"message": "APIå¯†é’¥ä½¿ç”¨æƒ…å†µå·²æˆåŠŸè®°å½•"}

# ==================== ç®¡ç†ç«¯ç‚¹ ====================

@app.get("/stats", response_model=Dict)
async def get_stats():
    """è·å–APIå¯†é’¥ç»Ÿè®¡ä¿¡æ¯"""
    return {
        "last_update": last_update_time.isoformat(),
        "failed_keys_by_source": {source: len(keys) for source, keys in failed_keys_cache.items()},
        "total_failed_keys": sum(len(keys) for keys in failed_keys_cache.values()),
        "detailed_stats": dict(stats_cache)
    }

@app.post("/refresh_cache")
async def refresh_cache():
    """æ‰‹åŠ¨åˆ·æ–°å¤±è´¥å¯†é’¥ç¼“å­˜"""
    logger.info("æ”¶åˆ°æ‰‹åŠ¨åˆ·æ–°ç¼“å­˜è¯·æ±‚")
    try:
        update_failed_keys_cache()
        return {"message": "ç¼“å­˜åˆ·æ–°æˆåŠŸ", "timestamp": datetime.now().isoformat()}
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨åˆ·æ–°ç¼“å­˜å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ·æ–°ç¼“å­˜å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8002, log_level="info") 